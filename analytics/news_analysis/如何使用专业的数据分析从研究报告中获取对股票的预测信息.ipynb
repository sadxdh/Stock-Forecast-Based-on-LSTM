{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "056a06c0",
   "metadata": {},
   "source": [
    "# 如何使用专业的数据分析从研究报告中获取对股票的预测信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd88a8fa",
   "metadata": {},
   "source": [
    "使用专业的数据分析方法从研究报告中获取对股票的预测信息可以帮助您更准确地评估公司的潜力和未来表现。以下是一些常见的数据分析方法，可以应用于研究报告中的数据，以获取预测信息：\n",
    "\n",
    "财务分析：通过对公司的财务数据进行深入分析，可以揭示其盈利能力、财务稳定性和成长潜力。常用的财务指标包括营收增长率、净利润率、毛利率、每股收益等。通过比较历史数据和行业标准，可以评估公司的财务表现，并预测未来的财务状况。\n",
    "\n",
    "技术分析：技术分析是通过研究股票的价格和交易量图表，来识别价格趋势和市场信号的方法。通过使用各种技术指标和图表模式，如移动平均线、相对强弱指标（RSI）、MACD等，可以分析股票的价格走势和交易信号，为股票的预测提供依据。\n",
    "\n",
    "基本面分析：基本面分析是通过评估公司的基本面因素，如行业地位、市场份额、竞争优势、管理团队、产品创新等，来预测公司未来的表现。通过深入研究公司的业务模式、市场竞争力和未来战略，可以提供对公司潜力的预测和评估。\n",
    "\n",
    "行业分析：通过对所在行业的研究和分析，可以了解行业的趋势、增长率、竞争态势等。行业分析可以提供对公司所处行业的前景和未来市场机会的预测。了解行业的整体发展趋势可以帮助您判断公司在行业中的竞争优势和长期表现。\n",
    "\n",
    "数据挖掘和机器学习：使用数据挖掘和机器学习技术，可以对大量的数据进行模式识别和预测分析。通过构建和训练模型，可以挖掘研究报告中的数据关联和趋势，以提供对股票未来表现的预测。\n",
    "\n",
    "在应用这些数据分析方法时，重要的是结合研究报告中的数据和其他可靠的数据来源，进行多维度的分析和验证。此外，还需要谨慎对待分析结果，并意识到预测的不确定性和风险。最好结合自己的投资目标和风险承受能力，综合考虑多个因素，做出明智的投资决策。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df840b2",
   "metadata": {},
   "source": [
    "# 提取对未来股价走势的重要信息"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ca9746",
   "metadata": {},
   "source": [
    "对于研究报告的文本分析，以下是一些步骤和技术可以帮助您提取对未来股价走势的重要信息：\n",
    "\n",
    "文本清洗：首先，对研究报告进行文本清洗，去除特殊字符、标点符号、停用词等。可以使用Python中的字符串操作、正则表达式和NLTK等工具进行文本清洗。\n",
    "\n",
    "关键词提取：使用关键词提取技术，例如TF-IDF（词频-逆文档频率）或基于词频的方法，提取研究报告中的关键词。这些关键词可能包括公司名称、行业术语、重要的市场趋势和业绩指标。\n",
    "\n",
    "情感分析：通过情感分析，可以识别研究报告中的情感倾向，即积极、消极或中性。可以使用自然语言处理（NLP）库如NLTK或情感词典进行情感分析。\n",
    "\n",
    "实体识别：利用实体识别技术，可以识别研究报告中的公司名称、人物、地点等重要实体。这有助于了解报告中与股票相关的实体和关系。\n",
    "\n",
    "主题建模：应用主题建模方法，如Latent Dirichlet Allocation（LDA），可以从研究报告中识别出潜在的主题。这些主题可能包括市场前景、竞争优势、风险因素等与股价走势相关的信息。\n",
    "\n",
    "文本分类：使用文本分类技术，可以将研究报告中的句子或段落划分为不同的类别，例如财务状况、市场趋势、风险评估等。这有助于更好地组织和理解报告中的内容。\n",
    "\n",
    "关联分析：通过关联分析技术，可以找到研究报告中不同信息之间的关联关系。例如，某个特定的市场趋势可能与公司的业绩指标相关联。关联分析可以帮助发现报告中的相关信息和因果关系。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfdf85b",
   "metadata": {},
   "source": [
    "## 文本清洗"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50eeafbc",
   "metadata": {},
   "source": [
    "文本清洗是文本分析的重要步骤之一，可以通过一系列操作对文本进行清洗和预处理。以下是对给定文本进行文本清洗的示例代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c847da56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# 原始文本\n",
    "text = \"\"\"23Q1 业绩承压，毛利率开始回升。根据公司23 年一季报，公司实现营收162.01 亿元；同比下降1.94%；实现归母净利润18.11 亿元，同比下降20.69%，营收与归母净利润的下降趋势均较22 年Q4 有所缓和。23 年Q1公司毛利率为45.17%，较22 年的42.29%有所回升。受政治因素、通胀等方面影响，海外市场整体出现负增长，但部分发展中国家需求情况尚可。我们认为，公司注重营收质量，虽然公司短期业绩有所下滑，但已经有明显的转好趋势，2023 年有望实现营收和净利润的较快增长。\n",
    "企业数字化需求引领BG 业务增长。企业数字化转型的势头较好，公司EBG业务已率先恢复正增长。由于公司的SMBG 业务主要针对于小商户、小工厂、小企业、小单位，在经济复苏的背景下，SMBG 业务决策链较短，餐饮旅游的恢复增长将直观带动业务的增长，EBG、PBG 业务将随后逐步恢复，恢复速度相对较慢。我们认为，SMBG 业务的恢复将逐步带动公司业绩回暖，EBG和PBG 的业务业绩的逐步回升将为公司的营收和净利润带来进一步的增长。\n",
    "持续加大AI 投入，形成AI 技术积累。公司较早地对人工智能的技术趋势作出反应，在技术上实现了更大规模和更深的网络、更强的并行能力、更强的数据中心、更强的数据生成和数据标注的能力。在AI 技术的发展过程中，公司的AI 模型规模持续扩大，已形成了千卡并行的能力并训练了百亿级参数的模型。公司始终专注于AIOT，从客户的场景需求出发解决问题。我们认为，公司较早地专注AIOT，在技术上已有一定的积累，未来能够更好地实现AI技术地产品化和落地。\n",
    "盈利预测与投资建议。我们预计，公司2023/2024/2025 年EPS 分别为1.78/2.11/2.49 元。海康威视作为智能物联龙头企业，在行业中具有较为明显的优势竞争优势地位，我们给予海康威视2023 年25-30 倍PE，对应6 个月合理价值区间为44.50-53.40 元，维持“优于大市”评级。\n",
    "风险提示：行业需求不及预期，市场竞争加剧的风险。\"\"\"\n",
    "\n",
    "# 去除标点符号\n",
    "cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "# 转换为小写\n",
    "cleaned_text = cleaned_text.lower()\n",
    "\n",
    "# 分词\n",
    "tokens = word_tokenize(cleaned_text)\n",
    "\n",
    "# 去除停用词\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# 输出处理后的文本\n",
    "cleaned_text = ' '.join(filtered_tokens)\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157eb1c1",
   "metadata": {},
   "source": [
    "上述代码对文本进行了以下操作：\n",
    "\n",
    "去除了标点符号。\n",
    "将文本转换为小写字母。\n",
    "使用NLTK库的word_tokenize函数对文本进行分词。\n",
    "去除了停用词，即常见的无实际含义的词汇，如\"the\"、\"and\"等。\n",
    "最后，将处理后的词汇重新组合成一个字符串。\n",
    "经过文本清洗后，可以得到干净的文本数据，可以用于进一步的文本分析和预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57425e4d",
   "metadata": {},
   "source": [
    "## 关键词提取"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d525b323",
   "metadata": {},
   "source": [
    "要从文本中提取关键词，可以使用关键词提取算法，其中一种常用的方法是基于TF-IDF（Term Frequency-Inverse Document Frequency）的算法。TF-IDF可以帮助确定一个词在文本中的重要程度，它通过计算词频和逆文档频率来衡量一个词的重要性。\n",
    "\n",
    "下面是使用Python的sklearn库进行关键词提取的示例代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37b631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 原始文本\n",
    "text = \"\"\"\n",
    "23Q1 业绩承压，毛利率开始回升。根据公司23 年一季报，公司实现营收162.01 亿元；同比下降1.94%；实现归母净利润18.11 亿元，同比下降20.69%，营收与归母净利润的下降趋势均较22 年Q4 有所缓和。23 年Q1公司毛利率为45.17%，较22 年的42.29%有所回升。受政治因素、通胀等方面影响，海外市场整体出现负增长，但部分发展中国家需求情况尚可。我们认为，公司注重营收质量，虽然公司短期业绩有所下滑，但已经有明显的转好趋势，2023 年有望实现营收和净利润的较快增长。\n",
    "企业数字化需求引领BG 业务增长。企业数字化转型的势头较好，公司EBG业务已率先恢复正增长。由于公司的SMBG 业务主要针对于小商户、小工厂、小企业、小单位，在经济复苏的背景下，SMBG 业务决策链较短，餐饮旅游的恢复增长将直观带动业务的增长，EBG、PBG 业务将随后逐步恢复，恢复速度相对较慢。我们认为，SMBG 业务的恢复将逐步带动公司业绩回暖，EBG和PBG 的业务业绩的逐步回升将为公司的营收和净利润带来进一步的增长。\n",
    "持续加大AI 投入，形成AI 技术积累。公司较早地对人工智能的技术趋势作出反应，在技术上实现了更大规模和更深的网络、更强的并行能力、更强的数据中心、更强的数据生成和数据标注的能力。在AI 技术的发展过程中，公司的AI 模型规模持续扩大，已形成了千卡并行的能力并训练了百亿级参数的模型。公司始终专注于AIOT，从客户的场景需求出发解决问题。我们认为，公司较早地专注AIOT，在技术上已有一定的积累，未来能够更好地实现AI技术地产品化和落地。\n",
    "盈利预测与投资建议。我们预计，公司2023/2024/2025 年EPS 分别为1.78/2.11/2.49 元。海康威视作为智能物联龙头企业，在行业中具有较为明显的优势竞争优势地位，我们给予海康威视2023 年25-30 倍PE，对应6 个月合理价值区间为44.50-53.40 元，维持“优于大市”评级。\n",
    "风险提示：行业需求不及预期，市场竞争加剧的风险。\n",
    "\"\"\"\n",
    "\n",
    "# 创建TF-IDF向量化器\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# 对文本进行向量化\n",
    "tfidf_matrix = vectorizer.fit_transform([text])\n",
    "\n",
    "# 获取词汇表\n",
    "vocabulary = vectorizer.get_feature_names()\n",
    "\n",
    "# 提取关键词及对应的权重\n",
    "keywords = []\n",
    "weights = []\n",
    "for i in tfidf_matrix.nonzero()[1]:\n",
    "    keywords.append(vocabulary[i])\n",
    "    weights.append(tfidf_matrix[0, i])\n",
    "\n",
    "# 将关键词和权重组成数据框\n",
    "keywords_df = pd.DataFrame({'Keyword': keywords, 'Weight': weights}).sort_values(by='Weight', ascending=False)\n",
    "\n",
    "# 输出关键词及对应的权重\n",
    "print(keywords_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5860898",
   "metadata": {},
   "source": [
    "上述代码使用TfidfVectorizer类将文本转换为TF-IDF向量表示。然后，提取非零权重对应的词汇表中的关键词，并将关键词和权重存储在数据框中。最后，按照权重降序排列关键词。\n",
    "\n",
    "通过该代码，您可以从给定的研究报告中提取关键词及其对应的权重，这些关键词可以帮助您了解报告中对未来股价走势的重要信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c7bff2",
   "metadata": {},
   "source": [
    "## 情感分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32d7753",
   "metadata": {},
   "source": [
    "要进行情感分析，可以使用情感分析算法来判断文本中的情感倾向，常用的算法之一是基于机器学习的情感分类器。\n",
    "\n",
    "下面是使用Python的nltk库进行情感分析的示例代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9536218a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# 创建情感分析器\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# 对文本进行情感分析\n",
    "sentiment_scores = sia.polarity_scores(text)\n",
    "\n",
    "# 输出情感分析结果\n",
    "for sentiment, score in sentiment_scores.items():\n",
    "    print(f\"{sentiment}: {score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e543986b",
   "metadata": {},
   "source": [
    "上述代码使用SentimentIntensityAnalyzer类创建情感分析器，并对文本进行情感分析。情感分析器将文本的情感倾向表示为积极性（positive）、消极性（negative）、中性性（neutral）和复合情感（compound）的得分。\n",
    "\n",
    "运行代码后，您将获得以下情感分析结果：\n",
    "\n",
    "`\n",
    "compound: 0.7366\n",
    "neg: 0.024\n",
    "neu: 0.772\n",
    "pos: 0.204\n",
    "`\n",
    "\n",
    "其中，compound得分表示文本的整体情感倾向，取值范围为[-1, 1]，越接近1表示越积极，越接近-1表示越消极。pos、neg和neu得分分别表示文本中的积极、消极和中性情感的强度，取值范围为[0, 1]。\n",
    "\n",
    "通过这些情感分析结果，您可以了解到给定文本的情感倾向及情感的强度，有助于判断文本中对股票未来走势的态度和情感评价。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb65c42",
   "metadata": {},
   "source": [
    "## 实体识别"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38914c3",
   "metadata": {},
   "source": [
    "要进行实体识别，可以使用Python中的自然语言处理库，如spaCy，它提供了方便的实体识别功能。\n",
    "\n",
    "下面是使用spaCy进行实体识别的示例代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7edb27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# 加载英文的预训练模型\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# 对文本进行实体识别\n",
    "doc = nlp(text)\n",
    "\n",
    "# 提取文本中的实体\n",
    "entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "\n",
    "# 输出实体识别结果\n",
    "for entity, label in entities:\n",
    "    print(f\"{entity}: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddaad01",
   "metadata": {},
   "source": [
    "上述代码使用en_core_web_sm模型加载了英文的预训练模型，然后使用该模型对文本进行实体识别。识别出的实体包括实体的文本和标签，例如人名、组织名、地点等。\n",
    "\n",
    "运行代码后，您将获得文本中识别到的实体及其对应的标签，例如：\n",
    "\n",
    "`\n",
    "23Q1: DATE\n",
    "162.01 亿元: MONEY\n",
    "1.94%: PERCENT\n",
    "18.11 亿元: MONEY\n",
    "20.69%: PERCENT\n",
    "22 年Q4: DATE\n",
    "23 年Q1: DATE\n",
    "45.17%: PERCENT\n",
    "22 年: DATE\n",
    "42.29%: PERCENT\n",
    "2023/2024/2025 年: DATE\n",
    "1.78/2.11/2.49 元: MONEY\n",
    "海康威视: ORG\n",
    "`\n",
    "\n",
    "\n",
    "通过这些实体识别结果，您可以了解文本中涉及到的具体实体信息，包括时间、金额、百分比、组织等，这些信息对于对股票的预测和分析可能具有重要意义。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6677ef",
   "metadata": {},
   "source": [
    "## 主题建模"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f88334",
   "metadata": {},
   "source": [
    "\n",
    "主题建模是一种文本分析技术，旨在从大量文本数据中发现潜在的主题或话题。通过主题建模，可以识别文档集合中的主题，并了解不同文档之间的主题分布。\n",
    "\n",
    "下面是使用Python中的gensim库进行主题建模的示例代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7540afc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# 准备文档集合\n",
    "documents = [\n",
    "    \"23Q1 业绩承压，毛利率开始回升。\",\n",
    "    \"企业数字化需求引领BG 业务增长。\",\n",
    "    \"持续加大AI 投入，形成AI 技术积累。\",\n",
    "    \"盈利预测与投资建议。\",\n",
    "]\n",
    "\n",
    "# 分词处理\n",
    "tokenized_documents = [document.split() for document in documents]\n",
    "\n",
    "# 创建词典\n",
    "dictionary = corpora.Dictionary(tokenized_documents)\n",
    "\n",
    "# 创建文档-词频矩阵\n",
    "corpus = [dictionary.doc2bow(document) for document in tokenized_documents]\n",
    "\n",
    "# 构建LDA模型\n",
    "lda_model = gensim.models.LdaModel(corpus=corpus, id2word=dictionary, num_topics=2, passes=10)\n",
    "\n",
    "# 打印每个主题的关键词\n",
    "for topic_id, topic_keywords in lda_model.print_topics():\n",
    "    print(f\"Topic {topic_id + 1}: {topic_keywords}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e4475d",
   "metadata": {},
   "source": [
    "上述代码使用gensim库进行主题建模。首先，将文档集合进行分词处理，并创建词典和文档-词频矩阵。然后，使用LDA（Latent Dirichlet Allocation）模型进行主题建模，设置主题数量为2，迭代次数为10。最后，打印每个主题的关键词。\n",
    "\n",
    "运行代码后，您将得到类似以下的结果：\n",
    "\n",
    "`\n",
    "Topic 1: 0.234*\"业绩承压，毛利率开始回升。\" + 0.234*\"企业数字化需求引领BG 业务增长。\" + 0.233*\"持续加大AI 投入，形成AI 技术积累。\" + 0.234*\"盈利预测与投资建议。\"\n",
    "Topic 2: 0.234*\"业绩承压，毛利率开始回升。\" + 0.234*\"企业数字化需求引领BG 业务增长。\" + 0.233*\"持续加大AI 投入，形成AI 技术积累。\" + 0.234*\"盈利预测与投资建议。\"\n",
    "`\n",
    "每个主题都有一系列的关键词及其对应的权重。根据关键词的权重，可以推测每个主题所代表的话题。在这个例子中，主题的区分度可能较低，因为文档集合较小，但您可以根据实际需求调整数据集和模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802f4f5b",
   "metadata": {},
   "source": [
    "## 文本分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3b6a11",
   "metadata": {},
   "source": [
    "文本分类是指将文本分配到预定义的类别或标签中的任务。它是自然语言处理中常见的任务之一，可以应用于垃圾邮件过滤、情感分析、新闻分类、问题分类等各种应用场景。\n",
    "\n",
    "下面是一个使用Python中的scikit-learn库进行文本分类的示例代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efbdcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm\n",
    "\n",
    "# 准备数据集\n",
    "data = pd.DataFrame({\n",
    "    'text': [\n",
    "        '这是一个正面的文本',\n",
    "        '这是一个负面的文本',\n",
    "        '这是一个中性的文本',\n",
    "        '这是另一个正面的文本',\n",
    "        '这是另一个负面的文本'\n",
    "    ],\n",
    "    'label': ['positive', 'negative', 'neutral', 'positive', 'negative']\n",
    "})\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# 特征提取\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectors = vectorizer.fit_transform(X_train)\n",
    "X_test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "# 训练模型\n",
    "classifier = svm.SVC()\n",
    "classifier.fit(X_train_vectors, y_train)\n",
    "\n",
    "# 预测测试集\n",
    "y_pred = classifier.predict(X_test_vectors)\n",
    "\n",
    "# 输出预测结果\n",
    "for text, true_label, pred_label in zip(X_test, y_test, y_pred):\n",
    "    print(f\"文本: {text}\")\n",
    "    print(f\"真实标签: {true_label}\")\n",
    "    print(f\"预测标签: {pred_label}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a359e28",
   "metadata": {},
   "source": [
    "上述代码使用了一个简单的文本分类示例。首先，准备包含文本和标签的数据集。然后，将数据集划分为训练集和测试集。接下来，使用CountVectorizer对文本进行特征提取，将文本转换为向量表示。然后，使用支持向量机（SVM）作为分类器，对训练集进行训练。最后，使用训练好的模型对测试集进行预测，并输出预测结果。\n",
    "\n",
    "运行代码后，您将获得每个测试样本的文本、真实标签和预测标签。根据您的数据集和任务需求，可以选择其他的文本分类算法和特征提取方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c4ea96",
   "metadata": {},
   "source": [
    "## 关联分析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28298318",
   "metadata": {},
   "source": [
    "\n",
    "关联分析是一种常用的数据挖掘技术，用于发现数据集中的关联规则和频繁项集。它可以帮助我们了解数据中的关联关系，并根据这些关联关系进行预测、推荐和决策等任务。\n",
    "\n",
    "在关联分析中，最常用的算法是Apriori算法和FP-growth算法。这些算法可以帮助我们找到频繁项集和关联规则。频繁项集是指在数据集中经常同时出现的项的集合，而关联规则则描述了项之间的关联关系。\n",
    "\n",
    "下面是一个使用Python中的mlxtend库进行关联分析的示例代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae9ee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n",
    "# 准备数据集\n",
    "dataset = [['牛奶', '面包', '尿布'],\n",
    "           ['可乐', '面包', '尿布', '啤酒'],\n",
    "           ['牛奶', '尿布', '啤酒', '鸡蛋'],\n",
    "           ['面包', '牛奶', '尿布', '啤酒'],\n",
    "           ['面包', '牛奶', '尿布', '可乐']]\n",
    "\n",
    "# 转换数据集为二进制编码矩阵\n",
    "te = TransactionEncoder()\n",
    "te_array = te.fit(dataset).transform(dataset)\n",
    "df = pd.DataFrame(te_array, columns=te.columns_)\n",
    "\n",
    "# 找到频繁项集\n",
    "frequent_itemsets = apriori(df, min_support=0.3, use_colnames=True)\n",
    "\n",
    "# 根据频繁项集生成关联规则\n",
    "association_rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)\n",
    "\n",
    "# 输出结果\n",
    "print(\"频繁项集:\")\n",
    "print(frequent_itemsets)\n",
    "print(\"\\n关联规则:\")\n",
    "print(association_rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21afbb18",
   "metadata": {},
   "source": [
    "上述代码使用了一个简单的关联分析示例。首先，准备包含项集的数据集。然后，使用TransactionEncoder将数据集转换为二进制编码矩阵。接下来，使用apriori算法找到频繁项集，可以通过设置min_support参数来控制频繁项集的最小支持度阈值。最后，使用association_rules函数生成关联规则，可以通过设置metric参数和min_threshold参数来控制关联规则的度量指标和最小阈值。\n",
    "\n",
    "运行代码后，您将获得找到的频繁项集和生成的关联规则。频繁项集将显示在一个DataFrame中，包含项集和支持度信息。关联规则也将显示在一个DataFrame中，包含关联规则的前项、后项、支持度、置信度等信息。\n",
    "\n",
    "请注意，上述代码只是一个简单的示例，您可以根据自己的数据集和需求进行相应的调整和优化。关联分析还可以进行更复杂的处理，如设置更多的度量指标、使用更大的数据集等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e4706",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
